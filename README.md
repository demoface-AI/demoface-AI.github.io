<div align="center">
    <p>
    <img src="./src/title.png" alt="DepMamba Logo" style="weight: 200px;">
    </p>
     <p>
    Official project of demo samples for <br>
    <b><em>Emotional Face-to-Speech</em></b>
    </p>
     <a href="https://arxiv.org/abs/"><img src="https://img.shields.io/badge/arXiv-2409.15936-b31b1b.svg" alt="version"></a>    
    <a href="https://github.com/Jiaxin-Ye/DepMamba"><img src="https://img.shields.io/badge/Platform-linux-lightgrey" alt="version"></a>
    <a href="https://github.com/Jiaxin-Ye/DepMamba"><img src="https://img.shields.io/badge/Python-3.8+-orange" alt="version"></a>
    <a href="https://github.com/Jiaxin-Ye/DepMamba"><img src="https://img.shields.io/badge/PyTorch-2.0+-brightgreen" alt="python"></a>
    <a href="https://github.com/Jiaxin-Ye/DepMamba"><img src="https://img.shields.io/badge/License-MIT-red.svg" alt="mit"></a>
</div>


### ðŸ“° News
* 03/02/2025 Released the Web project.
  

### ðŸ“•Introduction

How much can we infer about an emotional voice solely from an expressive face? This intriguing question holds great potential for applications such as virtual character dubbing and aiding individuals with expressive language disorders. In this paper, we explore a new task, termed *emotional face-to-speech*, aiming to synthesize emotional speech directly from expressive facial cues.  To that end, we introduce **DEmoFace**, a novel generative framework that leverages a discrete diffusion transformer (DiT) with curriculum learning, built upon a multi-level neural audio codec. 


<div align="center">
    <p>
    <img src="./src/main.png" alt="DepMamba pipeline" style="weight: 350px;">
    </p>
</div>
